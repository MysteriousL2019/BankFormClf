{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaB_lF-q2lC6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.combine import SMOTETomek,SMOTEENN\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, matthews_corrcoef,auc\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load the txt file into the pandas dataframe"
      ],
      "metadata": {
        "id": "e1P8LvaVleWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_table(\"/content/traininingdata.txt\",sep=';')\n",
        "\n",
        "df_test = pd.read_table(\"/content/testdata.txt\",sep=';')\n",
        "print(df_train.shape)\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "distinct_value_counts = [df_train[column].nunique() for column in categorical_columns]\n",
        "\n",
        "print(distinct_value_counts)\n",
        "print(df_train.shape)\n",
        "print(Counter(df_train['y']))"
      ],
      "metadata": {
        "id": "NdnQeHAw2sZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing the Exploratory Data Analysis\n"
      ],
      "metadata": {
        "id": "_qHphudW4QGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head().append(df_train.tail())"
      ],
      "metadata": {
        "id": "YueC_czy23Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "id": "doazplfS4S3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "kfNBpEs73z60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head().append(df_test.tail())"
      ],
      "metadata": {
        "id": "J7F4fvC-3zsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.describe()"
      ],
      "metadata": {
        "id": "KYmCDhrC4e9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.info()"
      ],
      "metadata": {
        "id": "HKQgZFQO4g0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are no null values in the table"
      ],
      "metadata": {
        "id": "8Gt9yc2dy63l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "k1lv--3q4hv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isnull().sum()"
      ],
      "metadata": {
        "id": "fP6DK_4i6Wo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df_train.isnull(),cbar=False,cmap='viridis')"
      ],
      "metadata": {
        "id": "Z3bEvRXe4Ur1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df_test.isnull(),cbar=False,cmap='viridis')\n"
      ],
      "metadata": {
        "id": "gPWgijlT4U11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df_train.corr(),cbar=True,annot=True,cmap='Blues')"
      ],
      "metadata": {
        "id": "-E6jNqB97eTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next we can analyze strongly correlated columns such as pdays and previous:\n",
        "* How the two variables affect each other.\n",
        "* Whether the Cartesian product of the variables has an effect on the label.\n",
        "* Whether there is a possibility of crossover between the two variables."
      ],
      "metadata": {
        "id": "sge5sugcln82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x='pdays',y='previous',data=df_train)\n",
        "plt.xlabel('pdays')\n",
        "plt.ylabel('previous')"
      ],
      "metadata": {
        "id": "G0lqOClD7hse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='y',data=df_train,palette='hls')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AjNRjl2_x0yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_train['age'])\n"
      ],
      "metadata": {
        "id": "G1hulutzx7a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_train['balance'])"
      ],
      "metadata": {
        "id": "8Mh_QHPDx-o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_train['day'])"
      ],
      "metadata": {
        "id": "efVO6sKryBjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_train['duration'])"
      ],
      "metadata": {
        "id": "ZdSqwlIVyEkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_train['campaign'])"
      ],
      "metadata": {
        "id": "_-HyyXiDyLDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* construct the Table transformer"
      ],
      "metadata": {
        "id": "ewoZcP8DlsQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# feedforward and attention\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "def FeedForward(dim, mult = 4, dropout = 0.):\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, dim * mult * 2),\n",
        "        GEGLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dim * mult, dim)\n",
        "    )\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        dropped_attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', dropped_attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        return out, attn\n",
        "\n",
        "# transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads,\n",
        "        dim_head,\n",
        "        attn_dropout,\n",
        "        ff_dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout),\n",
        "                FeedForward(dim, dropout = ff_dropout),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, return_attn = False):\n",
        "        post_softmax_attns = []\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            attn_out, post_softmax_attn = attn(x)\n",
        "            post_softmax_attns.append(post_softmax_attn)\n",
        "\n",
        "            x = attn_out + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        if not return_attn:\n",
        "            return x\n",
        "\n",
        "        return x, torch.stack(post_softmax_attns)\n",
        "\n",
        "# numerical embedder\n",
        "\n",
        "class NumericalEmbedder(nn.Module):\n",
        "    def __init__(self, dim, num_numerical_types):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
        "        self.biases = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b n -> b n 1')\n",
        "        return x * self.weights + self.biases\n",
        "\n",
        "# main class\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        categories,\n",
        "        num_continuous,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads,\n",
        "        dim_head = 16,\n",
        "        dim_out = 1,\n",
        "        num_special_tokens = 2,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
        "        assert len(categories) + num_continuous > 0, 'input shape must not be null'\n",
        "\n",
        "        # categories related calculations\n",
        "\n",
        "        self.num_categories = len(categories)\n",
        "        self.num_unique_categories = sum(categories)\n",
        "\n",
        "        # create category embeddings table\n",
        "\n",
        "        self.num_special_tokens = num_special_tokens\n",
        "        total_tokens = self.num_unique_categories + num_special_tokens\n",
        "\n",
        "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
        "\n",
        "        if self.num_unique_categories > 0:\n",
        "            categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
        "            categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
        "            self.register_buffer('categories_offset', categories_offset)\n",
        "\n",
        "            # categorical embedding\n",
        "\n",
        "            self.categorical_embeds = nn.Embedding(total_tokens, dim)\n",
        "\n",
        "        # continuous\n",
        "\n",
        "        self.num_continuous = num_continuous\n",
        "\n",
        "        if self.num_continuous > 0:\n",
        "            self.numerical_embedder = NumericalEmbedder(dim, self.num_continuous)\n",
        "\n",
        "        # cls token\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        # transformer\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            heads = heads,\n",
        "            dim_head = dim_head,\n",
        "            attn_dropout = attn_dropout,\n",
        "            ff_dropout = ff_dropout\n",
        "        )\n",
        "\n",
        "        # to logits\n",
        "\n",
        "        self.to_logits = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_categ, x_numer, return_attn = False):\n",
        "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
        "\n",
        "        xs = []\n",
        "        if self.num_unique_categories > 0:\n",
        "            x_categ = x_categ + self.categories_offset\n",
        "\n",
        "            x_categ = self.categorical_embeds(x_categ)\n",
        "\n",
        "            xs.append(x_categ)\n",
        "\n",
        "        # add numerically embedded tokens\n",
        "        if self.num_continuous > 0:\n",
        "            x_numer = self.numerical_embedder(x_numer)\n",
        "\n",
        "            xs.append(x_numer)\n",
        "\n",
        "        # concat categorical and numerical\n",
        "\n",
        "        x = torch.cat(xs, dim = 1)\n",
        "\n",
        "        # append cls tokens\n",
        "        b = x.shape[0]\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim = 1)\n",
        "\n",
        "        # attend\n",
        "\n",
        "        x, attns = self.transformer(x, return_attn = True)\n",
        "\n",
        "        # get cls token\n",
        "\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # out in the paper is linear(relu(ln(cls)))\n",
        "\n",
        "        logits = self.to_logits(x)\n",
        "\n",
        "        if not return_attn:\n",
        "            return logits\n",
        "\n",
        "        return logits, attns"
      ],
      "metadata": {
        "id": "aZkzvajV7xCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using SMOTE to oversampling the traindata to enlarge the number of label '1'.\n",
        "* it interpolate the data using data near by"
      ],
      "metadata": {
        "id": "sb88ACJalzSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.combine import SMOTETomek,SMOTEENN\n",
        "from imblearn.over_sampling import KMeansSMOTE\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "d = {}\n",
        "le = LabelEncoder()\n",
        "categorical_columns = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n",
        "for col in categorical_columns:\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    d[col] = le.classes_\n",
        "for col in categorical_columns:\n",
        "    df_test[col] = le.fit_transform(df_test[col])\n",
        "    d[col] = le.classes_\n",
        "print('---')\n",
        "smo = SMOTE(sampling_strategy={'yes': 31937, 'no': 31937},random_state=2023)\n",
        "\n",
        "label_mapping = {\"no\": 0, \"yes\": 1}\n",
        "\n",
        "print(df_train)\n",
        "X_smo, y_smo = smo.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
        "print(Counter(y_smo))\n",
        "col_x = [i for i in df_train.columns if i not in ['y']]\n",
        "x_train = X_smo\n",
        "y_train = y_smo\n",
        "x_test, y_test = df_test[col_x], df_test.y\n",
        "print(y_train)\n",
        "print(y_test)\n",
        "y_test = y_test.map(label_mapping)\n",
        "y_train = y_train.map(label_mapping)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(df_train.shape)\n"
      ],
      "metadata": {
        "id": "P9lYyqOp8iC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "sns.heatmap(df_train.corr(),cbar=True,annot=True,cmap='Blues')"
      ],
      "metadata": {
        "id": "NWttPUA3Lznn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "s-_R1Fo28t3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prepare training and test data\n",
        "* Place discontinuous values first and continuous values second"
      ],
      "metadata": {
        "id": "MdfplQshmWYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res = [i for i in x_train.columns if i not in categorical_columns]\n",
        "\n",
        "x_train_reordered = x_train[categorical_columns + res]\n",
        "x_test_reordered = x_test[categorical_columns + res]\n",
        "print(x_train_reordered)\n",
        "x_train_tensor = torch.FloatTensor(x_train_reordered.values)\n",
        "print(x_train_tensor.shape)\n",
        "y_train_tensor = torch.LongTensor(y_train.values)\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,pin_memory=True)\n",
        "\n",
        "x_test_tensor = torch.FloatTensor(x_test_reordered.values)\n",
        "y_test_tensor = torch.LongTensor(y_test.values)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False,pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IeyEUt1-8veT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_continuous = len(res)\n",
        "model = FTTransformer(\n",
        "    categories = distinct_value_counts,      # tuple containing the number of unique values within each category\n",
        "    num_continuous = num_continuous,                # number of continuous values\n",
        "    dim = x_train.shape[1],                           # dimension, paper set at 32\n",
        "    dim_out = 1,                        # binary prediction, but could be anything\n",
        "    depth = 6,                          # depth, paper recommended 6\n",
        "    heads = 8,                          # heads, paper recommends 8\n",
        "    attn_dropout = 0.1,                 # post-attention dropout\n",
        "    ff_dropout = 0.1                    # feed forward dropout\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "HkVPGEKs8yNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Change the loss weight base on the number of each class"
      ],
      "metadata": {
        "id": "gX067DtsmdEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, weight=None):\n",
        "        super(WeightedBCEWithLogitsLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        weights = torch.zeros(targets.shape).to(device)\n",
        "        for i in range(targets.shape[0]):\n",
        "            for j in range(targets.shape[1]):\n",
        "                weights[i][j] = class_weights[int(targets[i][j])]\n",
        "        loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, weight=weights)\n",
        "        return loss\n",
        "# Create category weights\n",
        "class_weights = torch.tensor([1,4])  #\n",
        "\n"
      ],
      "metadata": {
        "id": "Tv1D-iEM81DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = WeightedBCEWithLogitsLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# training model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs_categorical = inputs[:, :len(categorical_columns)].long().to(device)\n",
        "        inputs_continuous = inputs[:, len(categorical_columns):].to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs_categorical, inputs_continuous)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 1 == 0 and i % 250 == 0:\n",
        "            print(\"epoch: {}, iteration: {}, loss: {}\".format(epoch, i, loss.item()))\n",
        "\n",
        "    # Evaluate on the test set at the end of each epoch\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_{epoch+1}.pth\")\n",
        "\n",
        "# save model file\n",
        "filename = 'best_model_TF_continue3.pkl'\n",
        "# using pickle\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Predictions on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    for inputs, _ in test_dataloader:\n",
        "        inputs_categorical = inputs[:, :len(categorical_columns)].long().to(device)\n",
        "        inputs_continuous = inputs[:, len(categorical_columns):].to(device)\n",
        "        outputs = model(inputs_categorical, inputs_continuous)\n",
        "        predicted = torch.round(torch.sigmoid(outputs.squeeze()))\n",
        "        predictions.extend(predicted.tolist())\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "XANO4a3V84Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'best_model_TF_with_weight_8000_data_120_epoch_.pkl'\n",
        "#\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "paDJh8bA85o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model file\n",
        "# filename = '/content/best_model_TF_with_weight_8000_data_120_epoch_.pkl'\n",
        "# # model = None\n",
        "# with open(filename, 'rb') as file:\n",
        "#     model = pickle.load(file)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    pred_probability = []\n",
        "    for inputs, _ in test_dataloader:\n",
        "        inputs_categorical = inputs[:, :len(categorical_columns)].long().to(device)\n",
        "        inputs_continuous = inputs[:, len(categorical_columns):].to(device)\n",
        "        outputs = model(inputs_categorical, inputs_continuous)\n",
        "        predicted = torch.round(torch.sigmoid(outputs.squeeze()))\n",
        "        predicted_probability = torch.sigmoid(outputs.squeeze())\n",
        "        predictions.extend(predicted.tolist())\n",
        "        pred_probability.extend(predicted_probability.tolist())\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "print(outputs)\n",
        "print(outputs.shape)\n",
        "print(type(predictions))\n",
        "print(len(predictions))\n",
        "print(type(outputs))\n",
        "print(outputs.shape)\n",
        "y_pred = predictions\n",
        "print(len(pred_probability))\n",
        "print(pred_probability)"
      ],
      "metadata": {
        "id": "ncdo3mU687SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import expit\n",
        "y_pred = predictions\n",
        "\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred_probability)\n",
        "auc_value = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_value:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "print(\"AUC Score:\", auc_value)\n",
        "\n",
        "y_pred = predictions\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Macro-average F-measure:\", macro_f1)\n",
        "\n",
        "micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "print(\"Micro-average F-measure:\", micro_f1)\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# %%"
      ],
      "metadata": {
        "id": "h3v4pqTD88n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('y_pred.npy',y_pred)"
      ],
      "metadata": {
        "id": "sPz56kW9ULtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('y_test.npy',y_test)\n"
      ],
      "metadata": {
        "id": "2oIX7mb5UvUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "sns.heatmap(pd.DataFrame(report_dict).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
        "\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Target Labels\")\n",
        "plt.title(\"Classification Report\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0RfRJ_zYIMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(3, 2.25))\n",
        "sns.heatmap(cm, annot=True, cmap='Reds', fmt='d', cbar=False)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "py86cN96YsTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label for label in report_dict.keys() if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
        "precision = [report_dict[label]['precision'] for label in labels]\n",
        "recall = [report_dict[label]['recall'] for label in labels]\n",
        "f1_score = [report_dict[label]['f1-score'] for label in labels]\n",
        "\n",
        "x = range(len(labels))\n",
        "width = 0.185\n",
        "\n",
        "plt.bar(x, precision, width, label='Precision')\n",
        "plt.bar([i + width for i in x], recall, width, label='Recall')\n",
        "plt.bar([i + 2 * width for i in x], f1_score, width, label='F1-Score')\n",
        "for i, val in enumerate(precision):\n",
        "    plt.text(i, val, round(val, 2), ha='center', va='bottom')\n",
        "for i, val in enumerate(recall):\n",
        "    plt.text(i+width, val, round(val, 2), ha='center', va='bottom')\n",
        "for i, val in enumerate(f1_score):\n",
        "    plt.text(i+2*width, val, round(val, 2), ha='center', va='bottom')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Classification Report')\n",
        "\n",
        "plt.xticks([i + width for i in x], labels)\n",
        "# plt.legend()\n",
        "plt.legend(loc='upper center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j5lJVK-kiuhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}